{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Federated Learning - MNIST Dataset\n",
    "#### Jithin Sasikumar\n",
    "\n",
    "This notebook discusses the training results that were obtained during **FL(GPU)** and **normal training**. Additionally, the convnet model and it's parameters used are also defined with the inferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convnet model architecture with groupnorm layers\n",
    "\n",
    "![model](images/model_flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MNIST - No of datapoints used by each model:**\n",
    "1. Normal trained model (Non-FL): $60,000$\n",
    "2. Random initialized model: $60,000$\n",
    "3. 70% pre-trained model: $59,800$\n",
    "5. 80% pre-trained model: $59,400$\n",
    "\n",
    "#### General important training parameters:\n",
    "**Normal training (Non-FL):**\n",
    "1. Epochs = $100$\n",
    "2. Batch_size = $128$\n",
    "3. Learning rate = $1e-2$\n",
    "4. X_train shape = $(60000, 28, 28, 1)$\n",
    "5. X_test shape (validation set) = $(10000, 28, 28, 1)$\n",
    "6. Optimizer = $SGD$\n",
    "7. Labels are integers, not **one-hot encoded**.\n",
    "\n",
    "**FL training:**\n",
    "1. Number of clients = $100 (100*600=60,000)$\n",
    "2. Rounds = $100$\n",
    "3. Learning rate = $1e-2$\n",
    "4. No of epochs per round = $20$\n",
    "5. Batch_size per epoch = $10$\n",
    "6. Prefetch buffer = $10$\n",
    "7. Optimizer = $SGD$\n",
    "8. Labels are integers, not **one-hot encoded**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FL vs Non-FL training: \n",
    "#### Validation accuracy plot of different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![val_acc_all](results/FL_vs_Non_FL/FL_nonFL_acuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For better visualization, \n",
    "Last 50 rounds,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![val_acc_50](results/FL_vs_Non_FL/FL_nonFL_acuracy_last_50r.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without random initialized model,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![val_acc_norand](results/FL_vs_Non_FL/FL_nonFL_acuracy_no_rand.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference:\n",
    "\n",
    "1. From the analysis, it can be inferred that pretraining doesn't improve the FL models much, as there is only some negligible difference in results comparitively (i.e) Randomly initialized models and pretrained models delivers similar outputs.\n",
    "2. Models irrespective of different starting points, all complete at similar ending points (FL vs Non-FL plot).\n",
    "3. Addition of normalization layers (groupnorm) in the network seems to improve the model performance. \n",
    "4. FL training is pretty comparble with normal training, even though FL dosn't outperform normal conventional trained models but it would get better in the future.\n",
    "5. The above inferences are listed based on the above results with **100 clients**, **100 rounds** and **SGD** for MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
